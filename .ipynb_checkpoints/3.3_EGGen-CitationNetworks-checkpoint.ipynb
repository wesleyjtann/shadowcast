{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable Conditional LSTM random walk graph GAN training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleyjtann/miniconda3/envs/eggen/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from eggen import utils\n",
    "from eggen.eggen_shadow import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import pickle\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_name):\n",
    "    \"\"\"Load a graph from a Numpy binary file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Name of the file to load.\n",
    "    Returns\n",
    "    -------\n",
    "    graph : dict\n",
    "        Dictionary that contains:\n",
    "            * 'A' : The adjacency matrix in sparse matrix format\n",
    "            * 'X' : The attribute matrix in sparse matrix format\n",
    "            * 'z' : The ground truth class labels\n",
    "            * Further dictionaries mapping node, class and attribute IDs\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.npz'):\n",
    "        file_name += '.npz'\n",
    "    with np.load(file_name) as loader:\n",
    "        loader = dict(loader)\n",
    "        A = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
    "                           loader['adj_indptr']), shape=loader['adj_shape'])\n",
    "\n",
    "        X = sp.csr_matrix((loader['attr_data'], loader['attr_indices'],\n",
    "                           loader['attr_indptr']), shape=loader['attr_shape'])\n",
    "\n",
    "        z = loader.get('labels')\n",
    "\n",
    "        graph = {\n",
    "            'A': A,\n",
    "            'X': X,\n",
    "            'z': z\n",
    "        }\n",
    "\n",
    "        idx_to_node = loader.get('idx_to_node')\n",
    "        if idx_to_node:\n",
    "            idx_to_node = idx_to_node.tolist()\n",
    "            graph['idx_to_node'] = idx_to_node\n",
    "\n",
    "        idx_to_attr = loader.get('idx_to_attr')\n",
    "        if idx_to_attr:\n",
    "            idx_to_attr = idx_to_attr.tolist()\n",
    "            graph['idx_to_attr'] = idx_to_attr\n",
    "\n",
    "        idx_to_class = loader.get('idx_to_class')\n",
    "        if idx_to_class:\n",
    "            idx_to_class = idx_to_class.tolist()\n",
    "            graph['idx_to_class'] = idx_to_class\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** N nodes 2995, E edges: 8416, K classes: 7 ****\n"
     ]
    }
   ],
   "source": [
    "# cora, cora_ml, citeseer, dblp, pubmed\n",
    "g = load_dataset('data/cora_ml.npz')\n",
    "A, X, z = g['A'], g['X'], g['z']\n",
    "\n",
    "# N= 4230, E= 5358, K=6\n",
    "print(\"**** N nodes {:}, E edges: {:}, K classes: {:} ****\".format(A.shape[0],\n",
    "                                                                   np.count_nonzero(A.todense()>0),\n",
    "                                                                   len(np.unique(z))\n",
    "                                                                  ))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# _A_obs, _X_obs, _z_obs = utils.load_npz('data/cora_ml.npz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_A_obs = A + A.T\n",
    "_A_obs[_A_obs > 1] = 1\n",
    "lcc = utils.largest_connected_components(_A_obs)\n",
    "_A_obs = _A_obs[lcc,:][:,lcc]\n",
    "_N = _A_obs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adj matrix and Class data (conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_list = np.stack((np.arange(len(z)), z), axis=-1) #(N nodes, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adj_M = A.todense() #(N nodes, N nodes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.count_nonzero(Adj_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_zero = np.argwhere(z==0)\n",
    "ind_four = np.argwhere(z==4)\n",
    "ind_five = np.argwhere(z==5)\n",
    "print(len(ind_zero))\n",
    "print(len(ind_four))\n",
    "print(len(ind_five))\n",
    "ind_remove = np.concatenate((ind_zero, ind_four, ind_five), axis=0).flatten()\n",
    "print(len(ind_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condlist_new3 = np.delete(cond_list, ind_remove, 0)\n",
    "# np.unique(new3_condlist[:,1], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_new3 = A.todense()\n",
    "\n",
    "\"\"\" Deleting rows and cols that are not relevant to the selected group \"\"\"\n",
    "A_new3 = np.delete(A_new3, ind_remove, 0)\n",
    "A_new3 = np.delete(A_new3, ind_remove, 1)\n",
    "\n",
    "A_new3 = sp.coo_matrix(A_new3)\n",
    "A_new3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A_new3\n",
    "cond_list = condlist_new3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import nxviz as nv\n",
    "\n",
    "G = nx.from_numpy_matrix(Adj_M)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Number of nodes in G: \" ,G.number_of_nodes())\n",
    "print(\"Number of edges in G: \" ,G.number_of_edges())\n",
    "print(\"Number of selfloops in G: \" ,G.number_of_selfloops())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from Pandas edgelist dataframe to Adjacency matrix for undirected graphs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adjtraining = nx.adjacency_matrix(G)\n",
    "Adjtraining.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Adjtraining = sp.csr_matrix(Adjtraining, dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjacency matrix of the largest connected components"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_A_obs = Adjtraining\n",
    "_A_obs = _A_obs + _A_obs.T # (597, 597)\n",
    "_A_obs[_A_obs > 1] = 1 # Max value of 1 (597, 597)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Reduce input graph to a subgraph where only the nodes in largest n_components are kept. \"\"\" \n",
    "\n",
    "lcc = utils.largest_connected_components(_A_obs) # len(lcc) = 584\n",
    "_A_obs = _A_obs[lcc,:][:,lcc] # (584, 584)\n",
    "_N = _A_obs.shape[0] # 584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "_A_obs = A\n",
    "_A_obs = A + A.T\n",
    "_A_obs[_A_obs > 1] = 1\n",
    "lcc = utils.largest_connected_components(_A_obs)\n",
    "_A_obs = _A_obs[lcc,:][:,lcc]\n",
    "_N = _A_obs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2810"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_nonlcc = np.delete(np.arange(len(cond_list)), lcc)\n",
    "lcc_condlist = np.delete(cond_list, ind_nonlcc, 0)\n",
    "len(lcc_condlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** N nodes 2810, E edges: 7981.0, K classes: 7 ****\n"
     ]
    }
   ],
   "source": [
    "print(\"**** N nodes {:}, E edges: {:}, K classes: {:} ****\".format(_N,\n",
    "                                                                  np.count_nonzero(_A_obs.todense())/2,\n",
    "                                                                  len(np.unique(lcc_condlist[:,1]))\n",
    "                                                                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(_A_obs.todense()))\n",
    "\n",
    "_A_obs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Get the subset of cond_list before reindexing from 0 \"\"\"\n",
    "all_nodes = np.arange(A.shape[0]) #(N nodes,)\n",
    "train_nodes = np.array(lcc) \n",
    "nontrain_nodes = np.setdiff1d(all_nodes, train_nodes) #(657,)\n",
    "nontrain = nontrain_nodes.tolist()\n",
    "subset_condlist = np.delete(cond_list, nontrain, 0)\n",
    "# reindexing the condlist subset\n",
    "subset_condlist[:,0] = np.arange(subset_condlist.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unique_conds, conds_counts = np.unique(subset_condlist[:,1], return_counts=True)\n",
    "unique_conds, conds_counts = np.unique(lcc_condlist[:,1], return_counts=True)\n",
    "print(unique_conds)\n",
    "print(conds_counts)\n",
    "\n",
    "def bins_labels(bins, **kwargs):\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "bins = range(len(unique_conds)+1)\n",
    "# plt.hist(subset_condlist[:,1], bins)  \n",
    "plt.hist(cond_list[:,1], bins)  \n",
    "bins_labels(bins, fontsize=15)\n",
    "# plt.title(\"CORA-ML\")\n",
    "plt.savefig('./image/cora-ml.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([348, 393, 440, 407, 781, 150, 291])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the edges into train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_share = 0.1\n",
    "test_share = 0.05\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the edges of the adjacency matrix into train, validation and test edges and randomly samples equal amount of validation and test non-edges. \n",
    "\"\"\"\n",
    "train_ones, val_ones, val_zeros, test_ones, test_zeros = utils.train_val_test_split_adjacency(_A_obs, val_share, test_share, seed, undirected=True, connected=True, asserts=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train ones',train_ones.shape)\n",
    "print('val_ones',val_ones.shape)\n",
    "print('val_zeros',val_zeros.shape)\n",
    "print('test_ones',test_ones.shape)\n",
    "print('test_zeros',test_zeros.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = sp.coo_matrix((np.ones(len(train_ones)),(train_ones[:,0], train_ones[:,1]))).tocsr()\n",
    "assert (train_graph.toarray() == train_graph.toarray().T).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adjustable parameters for training. \"\"\" \n",
    "# setting GPU id \n",
    "gpu_id = 1\n",
    "# setting the number of nodes\n",
    "_N = _A_obs.shape[0]\n",
    "# setting the length of random walks\n",
    "rw_len = 16 #5 \n",
    "# setting the training data batch size\n",
    "batch_size = 128\n",
    "# getting the number of departments\n",
    "n_conds=len(np.unique(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker = utils.RandomWalker(train_graph, cond_list, rw_len, p=1, q=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_rw = walker.cond_walk().__next__()\n",
    "print(\"Example random walk: \", example_rw[0][0])\n",
    "print(\"Example conditions: \", example_rw[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create our generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_gen=1e-7; l2_disc=5e-5 #1e-4 \n",
    "gen_lay=[40]; disc_lay=[30]  #[50]; disc_lay=[35]  \n",
    "lr_gen=0.0002; lr_disc=0.0002 #0.0001\n",
    "gen_iters=1; disc_iters=3\n",
    "discWdown_size=128; genWdown_size=128 #128; 128\n",
    "\n",
    "\n",
    "eggen = EGGen(_N, rw_len, walk_generator=walker, n_conds=n_conds, condition_dim=n_conds,\n",
    "              gpu_id=gpu_id, use_gumbel=True, gen_iters=gen_iters, disc_iters=disc_iters, \n",
    "              W_down_discriminator_size=discWdown_size, W_down_generator_size=genWdown_size,\n",
    "              l2_penalty_generator=l2_gen, l2_penalty_discriminator=l2_disc,generator_layers=gen_lay,\n",
    "              discriminator_layers=disc_lay, temp_start=5, lr_gen=lr_gen, lr_disc=lr_disc\n",
    "             ) #, plot_show=False\n",
    "\n",
    "# eggen = EGGen(_N, rw_len, walk_generator=walker, n_conds=n_conds, condition_dim=n_conds,\n",
    "#               gpu_id=gpu_id, use_gumbel=True, gen_iters=gen_iters, disc_iters=disc_iters,\n",
    "#               W_down_discriminator_size=128, W_down_generator_size=128, l2_penalty_generator=l2_gen,\n",
    "#               l2_penalty_discriminator=l2_disc,\n",
    "#               generator_layers=gen_lay, discriminator_layers=disc_lay, temp_start=5,\n",
    "#               lr_gen=lr_gen, lr_disc=lr_disc\n",
    "#              ) #, plot_show=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Define the stopping criterion\n",
    "stopping_criterion = \"val\"\n",
    "\n",
    "\n",
    "assert stopping_criterion in [\"val\", \"eo\"], \"Please set the desired stopping criterion.\"\n",
    "\n",
    "if stopping_criterion == \"val\": # use val criterion for early stopping\n",
    "    stopping = None\n",
    "elif stopping_criterion == \"eo\":  #use eo criterion for early stopping\n",
    "    stopping = 0.5 # set the target edge overlap here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_every = plot_every = 200 \n",
    "max_iters = 20000\n",
    "patience= 20 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eval_every = plot_every = 200 \n",
    "max_iters = 20000\n",
    "patience= 20 \n",
    "\n",
    "# train and save model to ./snapshots/\n",
    "log_dict = eggen.train(A_orig=_A_obs, val_ones=val_ones, val_zeros=val_zeros, stopping=stopping,\n",
    "                        eval_every=eval_every, plot_every=plot_every, max_patience=patience, max_iters=max_iters)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #### Save the training log\n",
    "## when changing the directory, remember to change directory in eggen.train() too\n",
    "save_directory = \"./snapshots_gencond\"\n",
    "model_name = \"gencond\"\n",
    "\n",
    "save_log = \"{}/log8_{}_maxiter{}_evalevery{}.pkl\".format(save_directory, model_name, max_iters, eval_every)\n",
    "f = open(save_log,\"wb\")\n",
    "pickle.dump(log_dict,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load saved log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved log file\n",
    "save_directory = \"./snapshots_gencond\" \n",
    "model_name = \"gencond\"\n",
    "max_iters_load = 20000 \n",
    "eval_every_load = 200 \n",
    "\n",
    "save_log = \"{}/log8_{}_maxiter{}_evalevery{}.pkl\".format(save_directory, model_name, max_iters_load, eval_every_load)\n",
    "\n",
    "with open(save_log, 'rb') as f:\n",
    "    log_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(log_dict['val_performances'])) * eval_every, \n",
    "         np.array(log_dict['val_performances'])[:,0], label=\"ROC-AUC\")\n",
    "plt.plot(np.arange(len(log_dict['val_performances'])) * eval_every,\n",
    "         np.array(log_dict['val_performances'])[:,1], label=\"Avg. Prec.\")\n",
    "\n",
    "plt.title(\"Validation Performance during Training\")\n",
    "plt.legend()\n",
    "plt.savefig('./image/top5_cond_val_perf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(log_dict['edge_overlaps'])/_A_obs.sum())\n",
    "plt.title(\"Edge Overlap during Training\")\n",
    "# plt.savefig('./image/top5_cond_EO.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(eggen.session, \"snapshots_gencond/model_best_19.ckpt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate random walks on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_many = eggen.generate_discrete(10000, conds, rw_len=rw_len, reuse=True)\n",
    "sample_many, explain_conds = eggen.generate_discrete(10000, rw_len=rw_len, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.get_default_session().run([sample_many, explain_conds], feed_dict={eggen.tau: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "sampleconds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "    if (_+1) % 500 == 0:\n",
    "        print(_+1)\n",
    "#     samples.append(sample_many.eval({eggen.tau: 0.5}))\n",
    "    sample, samplecond = tf.get_default_session().run([sample_many, explain_conds], feed_dict={eggen.tau: 0.5})\n",
    "    samples.append(sample)\n",
    "    sampleconds.append(samplecond)\n",
    "\n",
    "print(\"Shape of one fake walk: \", samples[0].shape)\n",
    "print(\"Shape of one explain cond: \", sampleconds[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble score matrix from the random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rws = np.array(samples).reshape([-1, rw_len])\n",
    "print(rws.shape)\n",
    "scores_matrix = utils.score_matrix_from_random_walks(rws, _N).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainable transition counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_walkconds = np.array(sampleconds).reshape([-1, rw_len])\n",
    "print(explain_walkconds.shape)\n",
    "\n",
    "# explain transition counts from RW samples\n",
    "explain_walkconds = np.array(explain_walkconds)\n",
    "bigram_conds = np.array(list(zip(explain_walkconds[:, :-1], explain_walkconds[:, 1:]))) # (50000, 2, 15)\n",
    "\n",
    "bigram_conds = np.transpose(bigram_conds, [0, 2, 1]) # (50000, 15, 2) 15 transitions of a walk\n",
    "bigram_conds = bigram_conds.reshape([-1, 2]) # (750000, 2) all transitions\n",
    "\n",
    "unique_trans, trans_counts = np.unique(bigram_conds, axis=0, return_counts=True)\n",
    "trans_percent = np.divide(trans_counts, np.sum(trans_counts))\n",
    "explainconds = np.array(list(zip(unique_trans, trans_counts, trans_percent))) # (25, 2)\n",
    "\n",
    "explainconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(trans_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute graph statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_select = sp.csr_matrix((np.ones(len(train_ones)), (train_ones[:,0], train_ones[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_select = train_graph\n",
    "A_select.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_graph = utils.graph_from_scores(scores_matrix, A_select.sum())\n",
    "plt.spy(sampled_graph, markersize=.2)\n",
    "# plt.savefig('./image/top5_cond_spy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(A_select, markersize=.2)\n",
    "# plt.savefig('./image/top5_original_spy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.edge_overlap(A_select.toarray(), sampled_graph)/A_select.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Retrieving node condition attr to get community statistics \"\"\"\n",
    "G_tmp = nx.from_pandas_edgelist(data_small, 'SENDER', 'RECEIVER') #, edge_attr=['SENDER DEPT']) # , 'RECEIVER DEPT'\n",
    "\n",
    "# Iterate over df rows and set the source and target nodes' attributes for each row:\n",
    "for _, row in data_small.iterrows():       \n",
    "    G_tmp.nodes[row['SENDER']]['attr'] = row['SENDER DEPT']\n",
    "    G_tmp.nodes[row['RECEIVER']]['attr'] = row['RECEIVER DEPT']\n",
    "    \n",
    "eval_condlist = list(nx.get_node_attributes(G_tmp,'attr').values())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# one-hot matrix of departments for train node ids\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "values = np.array(eval_condlist)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "# print(integer_encoded)\n",
    "\n",
    "# one hot encode\n",
    "encoded = to_categorical(integer_encoded) #cond_list[:,1])\n",
    "print(encoded.shape)\n",
    "\n",
    "all_nodes = np.arange(1005) # List of all node ids\n",
    "train_nodes = np.array(G.nodes) # nodes ids that were used in training the model\n",
    "nontrain_nodes = np.setdiff1d(all_nodes, train_nodes) #(all node ids) \\ (train node ids)\n",
    "nontrain = nontrain_nodes.tolist()\n",
    "\n",
    "encoded = np.delete(encoded, nontrain, 0)\n",
    "print(\"train encoded shape: \", encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.compute_graph_statistics(sampled_graph, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.compute_graph_statistics(A_select.toarray()) #, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eggen",
   "language": "python",
   "name": "eggen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
